{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import gzip\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn import cluster, manifold, linear_model, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import umap\n",
    "# Silence perf warning\n",
    "import warnings\n",
    "\n",
    "import nodevectors as graph2vec\n",
    "import csrgraph\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_blogcatalog(edgelist='./data/edges_blogcatalog.csv',\n",
    "                    labels='./data/group_edges_blogcatalog.csv',\n",
    "                    dedupe=True):\n",
    "    \"\"\"\n",
    "    Graph with cluster labels from blogcatalog\n",
    "    \n",
    "    Dedupe: Whether to deduplicate results (else some nodes have multilabels)\n",
    "    \"\"\"\n",
    "    G = nx.read_edgelist(edgelist, delimiter=',')\n",
    "    labels = pd.read_csv(labels, header=None)\n",
    "    labels.columns = ['node', 'label']\n",
    "    labels = labels.sort_values(by='node').reset_index(drop=True)\n",
    "    if dedupe:\n",
    "        labels = labels.loc[~labels.node.duplicated()\n",
    "                      ].reset_index(drop=True)\n",
    "    labels.node = labels.node.astype(int)\n",
    "    labels.label = labels.label.astype(int)\n",
    "    return G, labels\n",
    "\n",
    "def make_snap():\n",
    "    \"\"\"\n",
    "    Graph from university emails, clustered by departments\n",
    "    Data from http://snap.stanford.edu/data/email-Eu-core.html\n",
    "    Edge list Format\n",
    "    \"\"\"\n",
    "    res = requests.get('http://snap.stanford.edu/data/email-Eu-core.txt.gz', verify=False)\n",
    "    edges = gzip.GzipFile(fileobj=io.BytesIO(res.content))\n",
    "    edges = pd.read_csv(io.StringIO(edges.read().decode()), header=None, sep=' ')\n",
    "    edges.columns = ['src', 'dest']\n",
    "    # cluster labels per node\n",
    "    res = requests.get('http://snap.stanford.edu/data/email-Eu-core-department-labels.txt.gz', verify=False)\n",
    "    labels = gzip.GzipFile(fileobj=io.BytesIO(res.content))\n",
    "    labels = pd.read_csv(io.StringIO(labels.read().decode()), header=None, sep=' ')\n",
    "    labels.columns = ['node', 'cluster']\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from([(t.src, t.dest) for t in edges.itertuples()])\n",
    "    return G, pd.DataFrame({'node': list(G), 'label': labels.cluster})\n",
    "\n",
    "def cluster_graph(n_nodes, n_clusters, connections=1, drop_pct=0.1):\n",
    "    \"\"\"\n",
    "    Makes distinct complete subgraphs\n",
    "        connected by random paths\n",
    "        \n",
    "    n_nodes (int): number of nodes\n",
    "    n_clusters (int): number of clusters\n",
    "        This is also the number of disjoint subgraphs\n",
    "    connections (int): number of random connections \n",
    "        These join the disjoint subgraphs\n",
    "    \"\"\"\n",
    "    div = int(n_nodes / n_clusters)\n",
    "    subgraph_sizes = [div] * n_clusters\n",
    "    # last cluster has remainder nodes\n",
    "    subgraph_sizes[-1] = subgraph_sizes[-1] + (n_nodes % n_clusters)\n",
    "    # Make G from disjoint subgraphs\n",
    "    G = nx.complete_graph(subgraph_sizes[0])\n",
    "    for i in range(1, len(subgraph_sizes)):\n",
    "        G = nx.disjoint_union(G, nx.complete_graph(subgraph_sizes[i]))\n",
    "    # connecting paths\n",
    "    for i in range(connections):\n",
    "        while True:\n",
    "            c1, c2 = np.random.randint(n_nodes, size=2)\n",
    "            if G.has_edge(c1, c2):\n",
    "                continue\n",
    "            G.add_edge(c1, c2)\n",
    "            break\n",
    "    # Drop random edges\n",
    "    n_edges = len(G.edges)\n",
    "    to_remove=random.sample(G.edges(),\n",
    "                            k=int(n_edges * drop_pct))\n",
    "    G.remove_edges_from(to_remove)\n",
    "    # Generate labels\n",
    "    labels = []\n",
    "    for i in range(len(subgraph_sizes)):\n",
    "        labels.append([i] * subgraph_sizes[i])\n",
    "    labels = sum(labels, [])\n",
    "    assert len(labels) == n_nodes, f\"{labels}\"\n",
    "    assert len(set(labels)) == n_clusters, f\"{labels}\"\n",
    "    return G, pd.DataFrame({'node': list(G), 'label': pd.Series(labels)})\n",
    "\n",
    "def evalClusteringOnLabels(clusters, groupLabels, verbose=True):\n",
    "    results = []\n",
    "    results.append(metrics.adjusted_mutual_info_score(clusters, groupLabels))\n",
    "    results.append(metrics.adjusted_rand_score(clusters, groupLabels))\n",
    "    results.append(metrics.fowlkes_mallows_score(clusters, groupLabels))\n",
    "    if verbose:\n",
    "        print(\"adj. MI score:   {0:.2f}\".format(results[0]))\n",
    "        print(\"adj. RAND score: {0:.2f}\".format(results[1]))\n",
    "        print(\"F-M score:       {0:.2f}\".format(results[2]))\n",
    "    return np.array(results)\n",
    "\n",
    "def to_X(node_labels, embedder):\n",
    "    \"\"\"\n",
    "    Takes a series of node names and returns matrix of embeddings\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame.from_records(\n",
    "        node_labels.astype(type(list(G)[0])).apply(embedder.predict).values)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G, labels = make_blogcatalog(dedupe=True)\n",
    "# G, labels = cluster_graph(n_nodes=6000, n_clusters=75, connections=3000, drop_pct=0.80)\n",
    "G, labels = make_snap()\n",
    "\n",
    "y = labels.label\n",
    "n_clusters = y.nunique()\n",
    "\n",
    "# Gridsearch result table\n",
    "res = pd.DataFrame(columns=['method', 'params', 'traintime', \n",
    "                            'F1', 'F1_test', 'MI', 'RAND', 'F-M'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    labels.node, labels.label, test_size=0.10, \n",
    "    random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: refactor analysis into functions\n",
    "\n",
    "add link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Embedder: 1650.74\n",
      "Trained: 1655.37\n",
      "TOL: 1e-05, LEARNING_RATE: 0.1, embed: 16\n",
      "------------\n",
      "best CV score: 0.4822\n",
      "test score: 0.0062\n",
      "adj. MI score:   0.00\n",
      "adj. RAND score: -0.00\n",
      "F-M score:       0.14\n",
      "-------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for TOL in [0.00001]:\n",
    " for LEARNING_RATE in [0.1]:\n",
    "  for EMBED_SIZE in [128, 256]:\n",
    "    start_t = time.time()\n",
    "    embedder = graph2vec.Glove(\n",
    "        n_components=EMBED_SIZE,\n",
    "        tol=TOL,\n",
    "        max_epoch=100_000,\n",
    "        learning_rate=LEARNING_RATE, \n",
    "        max_loss=10.,\n",
    "    )\n",
    "    embedder.fit(G)\n",
    "    train_t = time.time()\n",
    "    print(f\"Fit Embedder: {time.time() - start_t:.2f}\")\n",
    "    logit = linear_model.LogisticRegressionCV(cv=5, scoring='f1_macro',\n",
    "                                              max_iter=3000,\n",
    "                                              solver='lbfgs',\n",
    "                                              multi_class='ovr')\n",
    "    X_full = to_X(labels.node, embedder=embedder)\n",
    "    scaler = StandardScaler().fit(X_full)\n",
    "    logit.fit(scaler.transform(to_X(X_train, embedder=embedder)), y_train)\n",
    "    score = logit.scores_[1].mean(axis=0).max()\n",
    "    print(f\"Trained: {time.time() - start_t:.2f}\")\n",
    "    print(f\"TOL: {TOL}, LEARNING_RATE: {LEARNING_RATE}, embed: {EMBED_SIZE}\"\n",
    "      \"\\n------------\")\n",
    "    print(f'best CV score: {score :.4f}')\n",
    "    test_score = metrics.f1_score(\n",
    "        y_true=y_test,\n",
    "        y_pred=logit.predict(scaler.transform(to_X(X_test, embedder=embedder))),\n",
    "        average='macro'\n",
    "    )\n",
    "    print(f\"test score: {test_score :.4f}\")\n",
    "    \n",
    "    umpagglo = cluster.AgglomerativeClustering(\n",
    "        n_clusters=n_clusters, \n",
    "        affinity='cosine', \n",
    "        linkage='average'\n",
    "    ).fit(X_full).labels_\n",
    "\n",
    "    x = evalClusteringOnLabels(umpagglo, labels.label)\n",
    "    print(\"-------------------\\n\\n\")\n",
    "\n",
    "    res = res.append({\n",
    "        'method': 'GLoVe', \n",
    "        'params':{\n",
    "            'TOL': TOL,\n",
    "            'LEARNING_RATE': LEARNING_RATE,\n",
    "        }, \n",
    "        'traintime': train_t - start_t, \n",
    "        'F1': score, \n",
    "        'F1_test': test_score,\n",
    "        'MI':x[0],\n",
    "        'RAND':x[1], \n",
    "        'F-M':x[2]}, \n",
    "        ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Embedder: 11.23\n",
      "Trained: 105.63\n",
      "Neighbors: 15, dist: 0.1, embed: 64\n",
      "------------\n",
      "best CV score: 0.8766\n",
      "test score: 0.5021\n",
      "adj. MI score:   0.66\n",
      "adj. RAND score: 0.56\n",
      "F-M score:       0.58\n",
      "-------------------\n",
      "\n",
      "\n",
      "Fit Embedder: 7.63\n",
      "Trained: 149.04\n",
      "Neighbors: 15, dist: 0.1, embed: 128\n",
      "------------\n",
      "best CV score: 0.8846\n",
      "test score: 0.5056\n",
      "adj. MI score:   0.66\n",
      "adj. RAND score: 0.55\n",
      "F-M score:       0.58\n",
      "-------------------\n",
      "\n",
      "\n",
      "Fit Embedder: 3.06\n",
      "Trained: 106.41\n",
      "Neighbors: 5, dist: 0.1, embed: 64\n",
      "------------\n",
      "best CV score: 0.8919\n",
      "test score: 0.5038\n",
      "adj. MI score:   0.65\n",
      "adj. RAND score: 0.57\n",
      "F-M score:       0.59\n",
      "-------------------\n",
      "\n",
      "\n",
      "Fit Embedder: 4.64\n",
      "Trained: 143.88\n",
      "Neighbors: 5, dist: 0.1, embed: 128\n",
      "------------\n",
      "best CV score: 0.8937\n",
      "test score: 0.4210\n",
      "adj. MI score:   0.65\n",
      "adj. RAND score: 0.53\n",
      "F-M score:       0.55\n",
      "-------------------\n",
      "\n",
      "\n",
      "Fit Embedder: 5.11\n",
      "Trained: 98.08\n",
      "Neighbors: 15, dist: 0.01, embed: 64\n",
      "------------\n",
      "best CV score: 0.8734\n",
      "test score: 0.4352\n",
      "adj. MI score:   0.65\n",
      "adj. RAND score: 0.58\n",
      "F-M score:       0.60\n",
      "-------------------\n",
      "\n",
      "\n",
      "Fit Embedder: 7.68\n",
      "Trained: 143.32\n",
      "Neighbors: 15, dist: 0.01, embed: 128\n",
      "------------\n",
      "best CV score: 0.8780\n",
      "test score: 0.5161\n",
      "adj. MI score:   0.66\n",
      "adj. RAND score: 0.58\n",
      "F-M score:       0.60\n",
      "-------------------\n",
      "\n",
      "\n",
      "Fit Embedder: 3.07\n",
      "Trained: 105.34\n",
      "Neighbors: 5, dist: 0.01, embed: 64\n",
      "------------\n",
      "best CV score: 0.8978\n",
      "test score: 0.4407\n",
      "adj. MI score:   0.64\n",
      "adj. RAND score: 0.58\n",
      "F-M score:       0.60\n",
      "-------------------\n",
      "\n",
      "\n",
      "Fit Embedder: 4.59\n",
      "Trained: 153.19\n",
      "Neighbors: 5, dist: 0.01, embed: 128\n",
      "------------\n",
      "best CV score: 0.8747\n",
      "test score: 0.5476\n",
      "adj. MI score:   0.65\n",
      "adj. RAND score: 0.56\n",
      "F-M score:       0.58\n",
      "-------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for MIN_DIST in [0.1, 0.01]:\n",
    " for N_NEIGHBORS in [15, 5]:\n",
    "  for EMBED_SIZE in [64, 128]:\n",
    "    start_t = time.time()\n",
    "    embedder = graph2vec.SKLearnEmbedder(\n",
    "        umap.UMAP,\n",
    "        n_neighbors=N_NEIGHBORS,\n",
    "        min_dist=MIN_DIST,\n",
    "        metric='euclidean',\n",
    "        n_components=EMBED_SIZE,\n",
    "    )\n",
    "    embedder.fit(G)\n",
    "    train_t = time.time()\n",
    "    print(f\"Fit Embedder: {time.time() - start_t:.2f}\")\n",
    "    logit = linear_model.LogisticRegressionCV(cv=5, scoring='f1_macro',\n",
    "                                              max_iter=3000,\n",
    "                                              solver='lbfgs',\n",
    "                                              multi_class='ovr')\n",
    "    X_full = to_X(labels.node, embedder=embedder)\n",
    "    scaler = StandardScaler().fit(X_full)\n",
    "    logit.fit(scaler.transform(to_X(X_train, embedder=embedder)), y_train)\n",
    "    score = logit.scores_[1].mean(axis=0).max()\n",
    "    print(f\"Trained: {time.time() - start_t:.2f}\")\n",
    "    print(f\"Neighbors: {N_NEIGHBORS}, dist: {MIN_DIST}, embed: {EMBED_SIZE}\"\n",
    "      \"\\n------------\")\n",
    "    print(f'best CV score: {score :.4f}')\n",
    "    test_score = metrics.f1_score(\n",
    "        y_true=y_test,\n",
    "        y_pred=logit.predict(scaler.transform(to_X(X_test, embedder=embedder))),\n",
    "        average='macro'\n",
    "    )\n",
    "    print(f\"test score: {test_score :.4f}\")\n",
    "    \n",
    "    umpagglo = cluster.AgglomerativeClustering(\n",
    "        n_clusters=n_clusters, \n",
    "        affinity='cosine', \n",
    "        linkage='average'\n",
    "    ).fit(X_full).labels_\n",
    "\n",
    "    x = evalClusteringOnLabels(umpagglo, labels.label)\n",
    "    print(\"-------------------\\n\\n\")\n",
    "\n",
    "    res = res.append({\n",
    "        'method': 'UMAP', \n",
    "        'params':{\n",
    "            'n_neigbors': N_NEIGHBORS,\n",
    "            'min_dist': MIN_DIST,\n",
    "            'embed_size': EMBED_SIZE,\n",
    "        }, \n",
    "        'traintime': train_t - start_t, \n",
    "        'F1': score, \n",
    "        'F1_test': test_score,\n",
    "        'MI':x[0], \n",
    "        'RAND':x[1], \n",
    "        'F-M':x[2]}, \n",
    "        ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making walks... Done, T=31.57\n",
      "Mapping Walk Names... Done, T=0.95\n",
      "Training W2V... Done, T=5.57\n",
      "Fit Embedder: 38.27\n",
      "Trained: 88.66\n",
      "best CV score: 0.8746\n",
      "test score: 0.5313\n",
      "adj. MI score:   0.63\n",
      "adj. RAND score: 0.51\n",
      "F-M score:       0.54\n",
      "-------------------\n",
      "\n",
      "\n",
      "Making walks... Done, T=30.92\n",
      "Mapping Walk Names... Done, T=0.92\n",
      "Training W2V... Done, T=5.45\n",
      "Fit Embedder: 37.32\n",
      "Trained: 90.13\n",
      "best CV score: 0.8477\n",
      "test score: 0.4928\n",
      "adj. MI score:   0.63\n",
      "adj. RAND score: 0.53\n",
      "F-M score:       0.55\n",
      "-------------------\n",
      "\n",
      "\n",
      "Making walks... Done, T=30.90\n",
      "Mapping Walk Names... Done, T=0.88\n",
      "Training W2V... Done, T=5.48\n",
      "Fit Embedder: 37.38\n",
      "Trained: 86.50\n",
      "best CV score: 0.8523\n",
      "test score: 0.4640\n",
      "adj. MI score:   0.64\n",
      "adj. RAND score: 0.55\n",
      "F-M score:       0.57\n",
      "-------------------\n",
      "\n",
      "\n",
      "Making walks... Done, T=32.91\n",
      "Mapping Walk Names... Done, T=0.90\n",
      "Training W2V... Done, T=5.56\n",
      "Fit Embedder: 39.42\n",
      "Trained: 101.89\n",
      "best CV score: 0.8551\n",
      "test score: 0.5693\n",
      "adj. MI score:   0.65\n",
      "adj. RAND score: 0.55\n",
      "F-M score:       0.58\n",
      "-------------------\n",
      "\n",
      "\n",
      "Making walks... Done, T=0.99\n",
      "Mapping Walk Names... Done, T=0.90\n",
      "Training W2V... Done, T=5.51\n",
      "Fit Embedder: 7.43\n",
      "Trained: 68.16\n",
      "best CV score: 0.8605\n",
      "test score: 0.5026\n",
      "adj. MI score:   0.65\n",
      "adj. RAND score: 0.55\n",
      "F-M score:       0.58\n",
      "-------------------\n",
      "\n",
      "\n",
      "Making walks... Done, T=32.82\n",
      "Mapping Walk Names... Done, T=0.91\n",
      "Training W2V... Done, T=5.52\n",
      "Fit Embedder: 39.29\n",
      "Trained: 96.05\n",
      "best CV score: 0.8292\n",
      "test score: 0.4759\n",
      "adj. MI score:   0.64\n",
      "adj. RAND score: 0.54\n",
      "F-M score:       0.57\n",
      "-------------------\n",
      "\n",
      "\n",
      "Making walks... Done, T=35.53\n",
      "Mapping Walk Names... Done, T=0.89\n",
      "Training W2V... Done, T=5.57\n",
      "Fit Embedder: 42.12\n",
      "Trained: 113.00\n",
      "best CV score: 0.8396\n",
      "test score: 0.5058\n",
      "adj. MI score:   0.62\n",
      "adj. RAND score: 0.51\n",
      "F-M score:       0.54\n",
      "-------------------\n",
      "\n",
      "\n",
      "Making walks... Done, T=35.67\n",
      "Mapping Walk Names... Done, T=0.89\n",
      "Training W2V... Done, T=5.27\n",
      "Fit Embedder: 41.88\n",
      "Trained: 114.78\n",
      "best CV score: 0.8424\n",
      "test score: 0.4829\n",
      "adj. MI score:   0.64\n",
      "adj. RAND score: 0.53\n",
      "F-M score:       0.56\n",
      "-------------------\n",
      "\n",
      "\n",
      "Making walks... Done, T=35.49\n",
      "Mapping Walk Names... Done, T=0.97\n",
      "Training W2V... Done, T=5.53\n",
      "Fit Embedder: 42.02\n",
      "Trained: 109.60\n",
      "best CV score: 0.8438\n",
      "test score: 0.5430\n",
      "adj. MI score:   0.63\n",
      "adj. RAND score: 0.53\n",
      "F-M score:       0.55\n",
      "-------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for WALKLEN in [80]: # l in paper\n",
    " for EPOCH in [10]: # r in paper\n",
    "  for N_WEIGHT in [0.3, 1., 3.]:\n",
    "   for R_WEIGHT in [0.3, 1., 3.]:\n",
    "    for WINDOW in [10]: # k in paper\n",
    "     for EMBED_SIZE in [128]: # d in paper\n",
    "      for NS_EXP in [0.75]: # default, not in paper\n",
    "       for NEGATIVE in [5]: # default, not in paper\n",
    "        start_t = time.time()\n",
    "        embedder = graph2vec.Node2Vec(\n",
    "            walklen=WALKLEN,\n",
    "            epochs=EPOCH,\n",
    "            return_weight=R_WEIGHT,\n",
    "            neighbor_weight=N_WEIGHT,\n",
    "            n_components=EMBED_SIZE,\n",
    "            w2vparams={'window': WINDOW,\n",
    "                       'negative': NEGATIVE, \n",
    "                       'iter': 5,\n",
    "                       'ns_exponent': NS_EXP,\n",
    "                       'batch_words': 128}\n",
    "        )\n",
    "        embedder.fit(G)\n",
    "        train_t = time.time()\n",
    "        print(f\"Fit Embedder: {time.time() - start_t:.2f}\")\n",
    "        logit = linear_model.LogisticRegressionCV(cv=5, scoring='f1_macro',\n",
    "                                                  max_iter=3000,\n",
    "                                                  solver='lbfgs',\n",
    "                                                  multi_class='ovr')\n",
    "        X_full = to_X(labels.node, embedder=embedder)\n",
    "        scaler = StandardScaler().fit(X_full)\n",
    "        logit.fit(scaler.transform(to_X(X_train, embedder=embedder)), y_train)\n",
    "        score = logit.scores_[1].mean(axis=0).max()\n",
    "        print(f\"Trained: {time.time() - start_t:.2f}\")\n",
    "        print(f'best CV score: {score :.4f}')\n",
    "        test_score = metrics.f1_score(\n",
    "            y_true=y_test,\n",
    "            y_pred=logit.predict(scaler.transform(to_X(X_test, embedder=embedder))),\n",
    "            average='macro'\n",
    "        )\n",
    "        print(f\"test score: {test_score :.4f}\")\n",
    "        \n",
    "        umpagglo = cluster.AgglomerativeClustering(\n",
    "            n_clusters=n_clusters, \n",
    "            affinity='cosine', \n",
    "            linkage='average'\n",
    "        ).fit(X_full).labels_\n",
    "\n",
    "        x = evalClusteringOnLabels(umpagglo, labels.label)\n",
    "        print(\"-------------------\\n\\n\")\n",
    "\n",
    "        res = res.append({\n",
    "            'method': 'Node2Vec', \n",
    "            'params':{\n",
    "                'walklen': WALKLEN,\n",
    "                'epochs': EPOCH,\n",
    "                'return_weight': R_WEIGHT,\n",
    "                'neighbor_weight': N_WEIGHT,\n",
    "                'window': WINDOW,\n",
    "                'size': EMBED_SIZE, \n",
    "                'negative': NEGATIVE, \n",
    "                'iter': EPOCH,\n",
    "                'ns_exponent': NS_EXP,\n",
    "                'batch_words': 128,\n",
    "            }, \n",
    "            'traintime': train_t - start_t, \n",
    "            'F1': score, \n",
    "            'F1_test': test_score,\n",
    "            'MI':x[0], \n",
    "            'RAND':x[1], \n",
    "            'F-M':x[2]}, \n",
    "            ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>params</th>\n",
       "      <th>traintime</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1_test</th>\n",
       "      <th>MI</th>\n",
       "      <th>RAND</th>\n",
       "      <th>F-M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>{'n_neigbors': 15, 'min_dist': 0.1, 'embed_siz...</td>\n",
       "      <td>11.227792</td>\n",
       "      <td>0.876551</td>\n",
       "      <td>0.502088</td>\n",
       "      <td>0.659598</td>\n",
       "      <td>0.563595</td>\n",
       "      <td>0.584050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>{'n_neigbors': 15, 'min_dist': 0.1, 'embed_siz...</td>\n",
       "      <td>7.625471</td>\n",
       "      <td>0.884603</td>\n",
       "      <td>0.505623</td>\n",
       "      <td>0.660412</td>\n",
       "      <td>0.554303</td>\n",
       "      <td>0.575888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>{'n_neigbors': 5, 'min_dist': 0.1, 'embed_size...</td>\n",
       "      <td>3.058904</td>\n",
       "      <td>0.891856</td>\n",
       "      <td>0.503792</td>\n",
       "      <td>0.649760</td>\n",
       "      <td>0.568570</td>\n",
       "      <td>0.588506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>{'n_neigbors': 5, 'min_dist': 0.1, 'embed_size...</td>\n",
       "      <td>4.643819</td>\n",
       "      <td>0.893720</td>\n",
       "      <td>0.421005</td>\n",
       "      <td>0.649087</td>\n",
       "      <td>0.531919</td>\n",
       "      <td>0.554372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>{'n_neigbors': 15, 'min_dist': 0.01, 'embed_si...</td>\n",
       "      <td>5.106231</td>\n",
       "      <td>0.873426</td>\n",
       "      <td>0.435216</td>\n",
       "      <td>0.649726</td>\n",
       "      <td>0.579084</td>\n",
       "      <td>0.598494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>{'n_neigbors': 15, 'min_dist': 0.01, 'embed_si...</td>\n",
       "      <td>7.677146</td>\n",
       "      <td>0.878013</td>\n",
       "      <td>0.516135</td>\n",
       "      <td>0.658134</td>\n",
       "      <td>0.579220</td>\n",
       "      <td>0.598613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>{'n_neigbors': 5, 'min_dist': 0.01, 'embed_siz...</td>\n",
       "      <td>3.071304</td>\n",
       "      <td>0.897846</td>\n",
       "      <td>0.440691</td>\n",
       "      <td>0.643065</td>\n",
       "      <td>0.576615</td>\n",
       "      <td>0.596205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>{'n_neigbors': 5, 'min_dist': 0.01, 'embed_siz...</td>\n",
       "      <td>4.586399</td>\n",
       "      <td>0.874679</td>\n",
       "      <td>0.547567</td>\n",
       "      <td>0.652783</td>\n",
       "      <td>0.560834</td>\n",
       "      <td>0.581127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Node2Vec</td>\n",
       "      <td>{'walklen': 80, 'epochs': 10, 'return_weight':...</td>\n",
       "      <td>38.265318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.629246</td>\n",
       "      <td>0.513860</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Node2Vec</td>\n",
       "      <td>{'walklen': 80, 'epochs': 10, 'return_weight':...</td>\n",
       "      <td>37.319187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.631756</td>\n",
       "      <td>0.526646</td>\n",
       "      <td>0.553741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Node2Vec</td>\n",
       "      <td>{'walklen': 80, 'epochs': 10, 'return_weight':...</td>\n",
       "      <td>37.375760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.641758</td>\n",
       "      <td>0.545139</td>\n",
       "      <td>0.572952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Node2Vec</td>\n",
       "      <td>{'walklen': 80, 'epochs': 10, 'return_weight':...</td>\n",
       "      <td>39.418198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.646896</td>\n",
       "      <td>0.550551</td>\n",
       "      <td>0.575971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Node2Vec</td>\n",
       "      <td>{'walklen': 80, 'epochs': 10, 'return_weight':...</td>\n",
       "      <td>7.427551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.647021</td>\n",
       "      <td>0.549490</td>\n",
       "      <td>0.576551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Node2Vec</td>\n",
       "      <td>{'walklen': 80, 'epochs': 10, 'return_weight':...</td>\n",
       "      <td>39.293765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.644646</td>\n",
       "      <td>0.543727</td>\n",
       "      <td>0.567916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Node2Vec</td>\n",
       "      <td>{'walklen': 80, 'epochs': 10, 'return_weight':...</td>\n",
       "      <td>42.123744</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.624107</td>\n",
       "      <td>0.511142</td>\n",
       "      <td>0.542393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Node2Vec</td>\n",
       "      <td>{'walklen': 80, 'epochs': 10, 'return_weight':...</td>\n",
       "      <td>41.883127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.640190</td>\n",
       "      <td>0.533306</td>\n",
       "      <td>0.558531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Node2Vec</td>\n",
       "      <td>{'walklen': 80, 'epochs': 10, 'return_weight':...</td>\n",
       "      <td>42.023455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.627425</td>\n",
       "      <td>0.525782</td>\n",
       "      <td>0.552886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      method                                             params  traintime  \\\n",
       "0       UMAP  {'n_neigbors': 15, 'min_dist': 0.1, 'embed_siz...  11.227792   \n",
       "1       UMAP  {'n_neigbors': 15, 'min_dist': 0.1, 'embed_siz...   7.625471   \n",
       "2       UMAP  {'n_neigbors': 5, 'min_dist': 0.1, 'embed_size...   3.058904   \n",
       "3       UMAP  {'n_neigbors': 5, 'min_dist': 0.1, 'embed_size...   4.643819   \n",
       "4       UMAP  {'n_neigbors': 15, 'min_dist': 0.01, 'embed_si...   5.106231   \n",
       "5       UMAP  {'n_neigbors': 15, 'min_dist': 0.01, 'embed_si...   7.677146   \n",
       "6       UMAP  {'n_neigbors': 5, 'min_dist': 0.01, 'embed_siz...   3.071304   \n",
       "7       UMAP  {'n_neigbors': 5, 'min_dist': 0.01, 'embed_siz...   4.586399   \n",
       "8   Node2Vec  {'walklen': 80, 'epochs': 10, 'return_weight':...  38.265318   \n",
       "9   Node2Vec  {'walklen': 80, 'epochs': 10, 'return_weight':...  37.319187   \n",
       "10  Node2Vec  {'walklen': 80, 'epochs': 10, 'return_weight':...  37.375760   \n",
       "11  Node2Vec  {'walklen': 80, 'epochs': 10, 'return_weight':...  39.418198   \n",
       "12  Node2Vec  {'walklen': 80, 'epochs': 10, 'return_weight':...   7.427551   \n",
       "13  Node2Vec  {'walklen': 80, 'epochs': 10, 'return_weight':...  39.293765   \n",
       "14  Node2Vec  {'walklen': 80, 'epochs': 10, 'return_weight':...  42.123744   \n",
       "15  Node2Vec  {'walklen': 80, 'epochs': 10, 'return_weight':...  41.883127   \n",
       "16  Node2Vec  {'walklen': 80, 'epochs': 10, 'return_weight':...  42.023455   \n",
       "\n",
       "          F1   F1_test        MI      RAND       F-M  \n",
       "0   0.876551  0.502088  0.659598  0.563595  0.584050  \n",
       "1   0.884603  0.505623  0.660412  0.554303  0.575888  \n",
       "2   0.891856  0.503792  0.649760  0.568570  0.588506  \n",
       "3   0.893720  0.421005  0.649087  0.531919  0.554372  \n",
       "4   0.873426  0.435216  0.649726  0.579084  0.598494  \n",
       "5   0.878013  0.516135  0.658134  0.579220  0.598613  \n",
       "6   0.897846  0.440691  0.643065  0.576615  0.596205  \n",
       "7   0.874679  0.547567  0.652783  0.560834  0.581127  \n",
       "8        NaN       NaN  0.629246  0.513860  0.544800  \n",
       "9        NaN       NaN  0.631756  0.526646  0.553741  \n",
       "10       NaN       NaN  0.641758  0.545139  0.572952  \n",
       "11       NaN       NaN  0.646896  0.550551  0.575971  \n",
       "12       NaN       NaN  0.647021  0.549490  0.576551  \n",
       "13       NaN       NaN  0.644646  0.543727  0.567916  \n",
       "14       NaN       NaN  0.624107  0.511142  0.542393  \n",
       "15       NaN       NaN  0.640190  0.533306  0.558531  \n",
       "16       NaN       NaN  0.627425  0.525782  0.552886  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.to_csv('umap_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
